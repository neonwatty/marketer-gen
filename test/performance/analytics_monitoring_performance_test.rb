# frozen_string_literal: true

require "test_helper"
require "benchmark"
require "json"
require "webmock/minitest"
require "concurrent"

class AnalyticsMonitoringPerformanceTest < ActiveSupport::TestCase
  include ActiveJob::TestHelper

  # Performance targets based on requirements
  PERFORMANCE_TARGETS = {
    # Dashboard performance targets
    dashboard_load_time_seconds: 3.0,
    dashboard_refresh_time_seconds: 2.0,
    concurrent_users_supported: 100,
    
    # API response time targets
    analytics_api_response_seconds: 2.0,
    metrics_api_response_seconds: 1.5,
    aggregation_api_response_seconds: 2.0,
    
    # Data processing targets
    high_volume_processing_per_second: 20000, # 1M+ data points daily = ~11.6/sec sustained, 20k/sec peak
    etl_pipeline_max_seconds: 300, # 5 minutes for full ETL run
    real_time_processing_delay_ms: 100,
    
    # WebSocket and real-time targets
    websocket_connection_establishment_ms: 500,
    websocket_message_latency_ms: 50,
    concurrent_websocket_connections: 200,
    
    # Background job processing targets
    job_processing_rate_per_second: 500,
    job_queue_max_size: 10000,
    alert_delivery_time_seconds: 60,
    
    # Database performance targets
    query_execution_max_ms: 100,
    complex_aggregation_max_seconds: 5.0,
    bulk_insert_per_second: 5000,
    
    # Memory and resource targets
    max_memory_increase_mb: 500,
    cpu_usage_threshold_percent: 80
  }.freeze

  def setup
    @performance_results = {}
    @start_time = Time.current
    @test_brand = brands(:one)
    @test_user = users(:one)
    
    # Setup test data
    setup_test_integrations
    
    # Disable actual HTTP requests
    WebMock.disable_net_connect!(allow_localhost: true)
    
    # Clear job queues
    clear_enqueued_jobs
    clear_performed_jobs
    
    puts "\n" + "="*100
    puts "ANALYTICS MONITORING PERFORMANCE TEST SUITE"
    puts "Testing analytics system performance with high-volume data scenarios"
    puts "Target: 1M+ daily data points, 100+ concurrent users, <3s dashboard load"
    puts "="*100
  end

  def teardown
    @end_time = Time.current
    WebMock.reset!
    WebMock.allow_net_connect!
    generate_comprehensive_performance_report
  end

  # =============================================================================
  # COMPREHENSIVE PERFORMANCE TEST SUITE
  # =============================================================================

  test "analytics monitoring comprehensive performance benchmark" do
    puts "\nExecuting comprehensive analytics monitoring performance tests..."
    
    # Test 1: High-Volume Data Processing Performance
    run_performance_test("High-Volume Data Processing", critical: true) do
      test_high_volume_data_processing_performance
    end
    
    # Test 2: Dashboard Load and Concurrent User Performance
    run_performance_test("Dashboard Performance & Concurrent Users", critical: true) do
      test_dashboard_performance_and_concurrent_users
    end
    
    # Test 3: Real-Time WebSocket Performance
    run_performance_test("Real-Time WebSocket Performance", critical: true) do
      test_websocket_performance_stress
    end
    
    # Test 4: ETL Pipeline Performance
    run_performance_test("ETL Pipeline Performance", critical: true) do
      test_etl_pipeline_performance_benchmark
    end
    
    # Test 5: Background Job Processing Performance
    run_performance_test("Background Job Processing", critical: true) do
      test_background_job_processing_performance
    end
    
    # Test 6: Database Query Optimization Performance
    run_performance_test("Database Query Optimization", critical: true) do
      test_database_query_optimization_performance
    end
    
    # Test 7: Alert System Scalability Performance
    run_performance_test("Alert System Scalability", critical: true) do
      test_alert_system_scalability_performance
    end
    
    # Test 8: API Response Time Performance
    run_performance_test("API Response Time Performance", critical: true) do
      test_api_response_time_performance
    end
    
    # Test 9: Memory and Resource Usage
    run_performance_test("Memory and Resource Usage", critical: false) do
      test_memory_and_resource_usage
    end
    
    # Test 10: Stress Testing and System Resilience
    run_performance_test("Stress Testing and Resilience", critical: false) do
      test_stress_and_resilience
    end
    
    # Analyze overall performance and generate recommendations
    analyze_comprehensive_performance_results
  end

  private

  # =============================================================================
  # HIGH-VOLUME DATA PROCESSING PERFORMANCE TESTS
  # =============================================================================

  def test_high_volume_data_processing_performance
    puts "  Testing high-volume data processing (1M+ data points simulation)..."
    
    # Mock high-volume API responses
    stub_high_volume_platform_responses
    
    # Test 1: Social Media Metrics Processing
    social_media_time = benchmark_data_processing("Social Media", 50000) do |batch_size|
      process_social_media_metrics_batch(batch_size)
    end
    
    # Test 2: Email Marketing Metrics Processing
    email_time = benchmark_data_processing("Email Marketing", 30000) do |batch_size|
      process_email_metrics_batch(batch_size)
    end
    
    # Test 3: Google Analytics Metrics Processing
    ga_time = benchmark_data_processing("Google Analytics", 100000) do |batch_size|
      process_google_analytics_batch(batch_size)
    end
    
    # Test 4: CRM Data Processing
    crm_time = benchmark_data_processing("CRM", 25000) do |batch_size|
      process_crm_data_batch(batch_size)
    end
    
    # Calculate overall processing rate
    total_records = 205000
    total_time = [social_media_time, email_time, ga_time, crm_time].max
    processing_rate = total_records / total_time
    
    puts "  Overall processing rate: #{processing_rate.round(2)} records/second"
    puts "  Daily capacity at this rate: #{(processing_rate * 86400).round(0)} records"
    
    assert processing_rate >= PERFORMANCE_TARGETS[:high_volume_processing_per_second],
           "High-volume processing too slow: #{processing_rate.round(2)} records/second"
    
    @performance_results[:high_volume_processing] = {
      processing_rate: processing_rate,
      daily_capacity: processing_rate * 86400,
      social_media_time: social_media_time,
      email_time: email_time,
      ga_time: ga_time,
      crm_time: crm_time
    }
  end

  def benchmark_data_processing(data_type, record_count)
    puts "    Processing #{record_count} #{data_type} records..."
    
    time = Benchmark.measure do
      # Process in batches for realistic performance
      batch_size = 1000
      batches = (record_count / batch_size.to_f).ceil
      
      batches.times do |batch_index|
        current_batch_size = [batch_size, record_count - (batch_index * batch_size)].min
        yield(current_batch_size)
      end
    end
    
    rate = record_count / time.real
    puts "    #{data_type}: #{rate.round(2)} records/second"
    
    time.real
  end

  def process_social_media_metrics_batch(batch_size)
    metrics = []
    batch_size.times do |i|
      metrics << {
        platform: ['facebook', 'instagram', 'twitter', 'linkedin'].sample,
        metric_type: ['reach', 'engagement', 'impressions', 'clicks'].sample,
        value: rand(1000..50000),
        date: rand(30.days).seconds.ago.to_date
      }
    end
    
    # Simulate processing
    SocialMediaMetric.insert_all(
      metrics.map { |m| m.merge(
        social_media_integration_id: @facebook_integration.id,
        created_at: Time.current,
        updated_at: Time.current
      )}
    )
  end

  def process_email_metrics_batch(batch_size)
    metrics = []
    batch_size.times do |i|
      metrics << {
        email_campaign_id: @email_campaign.id,
        metric_type: ['sent', 'delivered', 'opened', 'clicked'].sample,
        value: rand(100..5000),
        date: rand(30.days).seconds.ago.to_date,
        created_at: Time.current,
        updated_at: Time.current
      }
    end
    
    EmailMetric.insert_all(metrics)
  end

  def process_google_analytics_batch(batch_size)
    metrics = []
    batch_size.times do |i|
      metrics << {
        metric_name: ['sessions', 'pageviews', 'users', 'bounce_rate'].sample,
        metric_value: rand(100..10000).to_f,
        date: rand(30.days).seconds.ago.to_date,
        brand_id: @test_brand.id,
        created_at: Time.current,
        updated_at: Time.current
      }
    end
    
    GoogleAnalyticsMetric.insert_all(metrics)
  end

  def process_crm_data_batch(batch_size)
    leads = []
    batch_size.times do |i|
      leads << {
        crm_integration_id: @crm_integration.id,
        external_id: "perf_lead_#{i}_#{rand(10000)}",
        email: "perf_lead_#{i}_#{rand(10000)}@example.com",
        status: ['new', 'qualified', 'converted'].sample,
        source: ['website', 'social', 'email', 'referral'].sample,
        score: rand(0..100),
        created_at: Time.current,
        updated_at: Time.current
      }
    end
    
    CrmLead.insert_all(leads)
  end

  # =============================================================================
  # DASHBOARD PERFORMANCE AND CONCURRENT USER TESTS
  # =============================================================================

  def test_dashboard_performance_and_concurrent_users
    puts "  Testing dashboard performance and concurrent user support..."
    
    # Test 1: Dashboard Load Time
    dashboard_load_time = test_dashboard_load_performance
    
    # Test 2: Dashboard Data Refresh Performance
    refresh_time = test_dashboard_refresh_performance
    
    # Test 3: Concurrent User Simulation
    concurrent_performance = test_concurrent_dashboard_users
    
    @performance_results[:dashboard_performance] = {
      load_time: dashboard_load_time,
      refresh_time: refresh_time,
      concurrent_users: concurrent_performance
    }
  end

  def test_dashboard_load_performance
    puts "    Testing dashboard load performance..."
    
    # Pre-populate with realistic data
    create_realistic_analytics_data
    
    load_time = Benchmark.measure do
      # Simulate dashboard load by calling the controller action
      get analytics_dashboard_path(@test_brand)
      assert_response :success
    end
    
    puts "    Dashboard load time: #{load_time.real.round(3)}s"
    
    assert load_time.real <= PERFORMANCE_TARGETS[:dashboard_load_time_seconds],
           "Dashboard load too slow: #{load_time.real}s"
    
    load_time.real
  end

  def test_dashboard_refresh_performance
    puts "    Testing dashboard refresh performance..."
    
    refresh_time = Benchmark.measure do
      # Simulate AJAX refresh
      get analytics_dashboard_path(@test_brand), xhr: true, params: {
        time_range: "30d",
        refresh: true
      }
      assert_response :success
    end
    
    puts "    Dashboard refresh time: #{refresh_time.real.round(3)}s"
    
    assert refresh_time.real <= PERFORMANCE_TARGETS[:dashboard_refresh_time_seconds],
           "Dashboard refresh too slow: #{refresh_time.real}s"
    
    refresh_time.real
  end

  def test_concurrent_dashboard_users
    puts "    Testing concurrent dashboard user support..."
    
    concurrent_users = 50
    successful_requests = Concurrent::AtomicFixnum.new(0)
    failed_requests = Concurrent::AtomicFixnum.new(0)
    response_times = Concurrent::Array.new
    
    concurrent_time = Benchmark.measure do
      threads = []
      
      concurrent_users.times do |user_index|
        threads << Thread.new do
          begin
            request_time = Benchmark.measure do
              # Simulate different user requests
              case user_index % 4
              when 0
                get analytics_dashboard_path(@test_brand)
              when 1
                get analytics_dashboard_path(@test_brand), params: { time_range: "7d" }
              when 2
                get analytics_dashboard_path(@test_brand), xhr: true
              when 3
                get analytics_dashboard_path(@test_brand), params: { platform: "facebook" }
              end
            end
            
            if response.status == 200
              successful_requests.increment
              response_times << request_time.real
            else
              failed_requests.increment
            end
          rescue => e
            failed_requests.increment
            puts "      Concurrent user #{user_index} failed: #{e.message}"
          end
        end
      end
      
      threads.each(&:join)
    end
    
    success_rate = (successful_requests.value.to_f / concurrent_users * 100).round(2)
    avg_response_time = response_times.empty? ? 0 : (response_times.sum / response_times.size)
    
    puts "    Concurrent users: #{concurrent_users}"
    puts "    Success rate: #{success_rate}%"
    puts "    Average response time: #{avg_response_time.round(3)}s"
    puts "    Total concurrent test time: #{concurrent_time.real.round(3)}s"
    
    assert success_rate >= 95, "Success rate too low: #{success_rate}%"
    assert avg_response_time <= PERFORMANCE_TARGETS[:dashboard_load_time_seconds],
           "Average response time too slow: #{avg_response_time}s"
    
    {
      users: concurrent_users,
      success_rate: success_rate,
      avg_response_time: avg_response_time,
      total_time: concurrent_time.real
    }
  end

  # =============================================================================
  # WEBSOCKET PERFORMANCE TESTS
  # =============================================================================

  def test_websocket_performance_stress
    puts "  Testing WebSocket performance and stress scenarios..."
    
    # Test 1: WebSocket Connection Establishment
    connection_time = test_websocket_connection_performance
    
    # Test 2: Message Latency
    message_latency = test_websocket_message_latency
    
    # Test 3: Concurrent WebSocket Connections
    concurrent_ws = test_concurrent_websocket_connections
    
    # Test 4: High-Frequency Message Broadcasting
    broadcast_performance = test_websocket_broadcast_performance
    
    @performance_results[:websocket_performance] = {
      connection_time: connection_time,
      message_latency: message_latency,
      concurrent_connections: concurrent_ws,
      broadcast_performance: broadcast_performance
    }
  end

  def test_websocket_connection_performance
    puts "    Testing WebSocket connection establishment..."
    
    connection_times = []
    
    10.times do
      connection_time = Benchmark.measure do
        # Simulate WebSocket connection
        connect "/cable"
        subscribe_to_channel("AnalyticsDashboardChannel", brand_id: @test_brand.id)
        assert_subscription_confirmed
      end
      
      connection_times << connection_time.real * 1000 # Convert to milliseconds
    end
    
    avg_connection_time = connection_times.sum / connection_times.size
    puts "    Average connection time: #{avg_connection_time.round(2)}ms"
    
    assert avg_connection_time <= PERFORMANCE_TARGETS[:websocket_connection_establishment_ms],
           "WebSocket connection too slow: #{avg_connection_time.round(2)}ms"
    
    avg_connection_time
  end

  def test_websocket_message_latency
    puts "    Testing WebSocket message latency..."
    
    connect "/cable"
    subscribe_to_channel("AnalyticsDashboardChannel", brand_id: @test_brand.id)
    
    message_latencies = []
    
    10.times do
      start_time = Time.current
      
      # Trigger a real-time update
      ActionCable.server.broadcast("analytics_dashboard_#{@test_brand.id}", {
        type: "metrics_update",
        data: { reach: rand(1000..5000), engagement: rand(100..500) },
        timestamp: start_time.to_f
      })
      
      # Simulate message processing time
      sleep(0.001)
      
      end_time = Time.current
      latency = (end_time - start_time) * 1000
      message_latencies << latency
    end
    
    avg_latency = message_latencies.sum / message_latencies.size
    puts "    Average message latency: #{avg_latency.round(2)}ms"
    
    assert avg_latency <= PERFORMANCE_TARGETS[:websocket_message_latency_ms],
           "WebSocket message latency too high: #{avg_latency.round(2)}ms"
    
    avg_latency
  end

  def test_concurrent_websocket_connections
    puts "    Testing concurrent WebSocket connections..."
    
    concurrent_connections = 25 # Reduced for test environment
    successful_connections = Concurrent::AtomicFixnum.new(0)
    
    connection_time = Benchmark.measure do
      threads = []
      
      concurrent_connections.times do |i|
        threads << Thread.new do
          begin
            connect "/cable", headers: { "User-Id" => "perf_user_#{i}" }
            subscribe_to_channel("AnalyticsDashboardChannel", brand_id: @test_brand.id)
            
            if subscription_confirmed?
              successful_connections.increment
            end
          rescue => e
            puts "      Connection #{i} failed: #{e.message}"
          end
        end
      end
      
      threads.each(&:join)
    end
    
    success_rate = (successful_connections.value.to_f / concurrent_connections * 100).round(2)
    puts "    Concurrent connections: #{concurrent_connections}"
    puts "    Successful connections: #{successful_connections.value}"
    puts "    Success rate: #{success_rate}%"
    puts "    Total connection time: #{connection_time.real.round(3)}s"
    
    assert success_rate >= 95, "WebSocket connection success rate too low: #{success_rate}%"
    
    {
      total_connections: concurrent_connections,
      successful_connections: successful_connections.value,
      success_rate: success_rate,
      connection_time: connection_time.real
    }
  end

  def test_websocket_broadcast_performance
    puts "    Testing WebSocket broadcast performance..."
    
    # Setup multiple connections
    5.times do |i|
      connect "/cable", headers: { "User-Id" => "broadcast_user_#{i}" }
      subscribe_to_channel("AnalyticsDashboardChannel", brand_id: @test_brand.id)
    end
    
    broadcast_count = 100
    broadcast_time = Benchmark.measure do
      broadcast_count.times do |i|
        ActionCable.server.broadcast("analytics_dashboard_#{@test_brand.id}", {
          type: "metrics_update",
          data: { metric_id: i, value: rand(1000) },
          timestamp: Time.current.to_f
        })
      end
    end
    
    broadcasts_per_second = broadcast_count / broadcast_time.real
    puts "    Broadcast performance: #{broadcasts_per_second.round(2)} broadcasts/second"
    
    {
      broadcasts: broadcast_count,
      time: broadcast_time.real,
      rate: broadcasts_per_second
    }
  end

  # =============================================================================
  # ETL PIPELINE PERFORMANCE TESTS
  # =============================================================================

  def test_etl_pipeline_performance_benchmark
    puts "  Testing ETL pipeline performance..."
    
    # Test 1: Complete ETL Pipeline Execution
    full_pipeline_time = test_full_etl_pipeline_execution
    
    # Test 2: Individual ETL Component Performance
    component_performance = test_etl_component_performance
    
    # Test 3: Real-Time ETL Processing
    realtime_performance = test_realtime_etl_performance
    
    # Test 4: ETL Error Recovery Performance
    error_recovery = test_etl_error_recovery_performance
    
    @performance_results[:etl_pipeline] = {
      full_pipeline_time: full_pipeline_time,
      component_performance: component_performance,
      realtime_performance: realtime_performance,
      error_recovery: error_recovery
    }
  end

  def test_full_etl_pipeline_execution
    puts "    Testing complete ETL pipeline execution..."
    
    # Mock all data sources
    stub_comprehensive_etl_data_sources
    
    pipeline_time = Benchmark.measure do
      perform_enqueued_jobs do
        # Execute all ETL jobs
        Etl::GoogleAnalyticsHourlyJob.perform_now
        Etl::SocialMediaRealTimeJob.perform_now(@facebook_integration.id)
        Etl::DataNormalizationJob.perform_now
      end
    end
    
    puts "    Full ETL pipeline time: #{pipeline_time.real.round(2)}s"
    
    assert pipeline_time.real <= PERFORMANCE_TARGETS[:etl_pipeline_max_seconds],
           "ETL pipeline too slow: #{pipeline_time.real}s"
    
    # Verify data was processed
    assert GoogleAnalyticsMetric.exists?, "Google Analytics data not processed"
    assert SocialMediaMetric.exists?, "Social Media data not processed"
    
    pipeline_time.real
  end

  def test_etl_component_performance
    puts "    Testing individual ETL component performance..."
    
    components = {}
    
    # Test data extraction
    extraction_time = Benchmark.measure do
      service = Etl::BaseEtlService.new("performance_test", "test_run_#{Time.current.to_i}")
      result = service.extract_data
      assert result.success?, "Data extraction should succeed"
    end
    components[:extraction] = extraction_time.real
    
    # Test data transformation
    transformation_time = Benchmark.measure do
      service = Etl::DataTransformationRules.new
      raw_data = generate_etl_test_data
      result = service.transform_all_platforms(raw_data)
      assert result.success?, "Data transformation should succeed"
    end
    components[:transformation] = transformation_time.real
    
    # Test data loading
    loading_time = Benchmark.measure do
      service = Etl::BaseEtlService.new("performance_test", "load_test_#{Time.current.to_i}")
      transformed_data = generate_transformed_etl_data
      result = service.load_data(transformed_data)
      assert result.success?, "Data loading should succeed"
    end
    components[:loading] = loading_time.real
    
    puts "    Extraction: #{components[:extraction].round(3)}s"
    puts "    Transformation: #{components[:transformation].round(3)}s"
    puts "    Loading: #{components[:loading].round(3)}s"
    
    components
  end

  def test_realtime_etl_performance
    puts "    Testing real-time ETL processing performance..."
    
    event_count = 1000
    processing_times = []
    
    event_count.times do |i|
      event_data = {
        type: "social_media_post",
        platform: "facebook",
        data: { post_id: "perf_post_#{i}", engagement: rand(10..100) }
      }
      
      processing_time = Benchmark.measure do
        Analytics::SocialMediaIntegrationService.new(@test_brand).process_real_time_event(event_data)
      end
      
      processing_times << processing_time.real * 1000 # Convert to milliseconds
    end
    
    avg_processing_time = processing_times.sum / processing_times.size
    processing_rate = 1000.0 / (processing_times.sum / processing_times.size)
    
    puts "    Real-time processing: #{avg_processing_time.round(2)}ms average"
    puts "    Processing rate: #{processing_rate.round(2)} events/second"
    
    assert avg_processing_time <= PERFORMANCE_TARGETS[:real_time_processing_delay_ms],
           "Real-time processing too slow: #{avg_processing_time.round(2)}ms"
    
    {
      avg_processing_time: avg_processing_time,
      processing_rate: processing_rate,
      events_processed: event_count
    }
  end

  def test_etl_error_recovery_performance
    puts "    Testing ETL error recovery performance..."
    
    # Simulate ETL failures and recovery
    recovery_time = Benchmark.measure do
      # Create failing pipeline run
      pipeline_run = EtlPipelineRun.create!(
        pipeline_name: "error_recovery_test",
        run_id: "error_test_#{Time.current.to_i}",
        status: "failed",
        error_message: "Simulated error for testing"
      )
      
      # Test recovery mechanism
      service = Etl::PipelineHealthMonitorJob.new
      service.perform
      
      # Verify recovery was attempted
      pipeline_run.reload
    end
    
    puts "    Error recovery time: #{recovery_time.real.round(3)}s"
    
    recovery_time.real
  end

  # =============================================================================
  # BACKGROUND JOB PROCESSING PERFORMANCE TESTS
  # =============================================================================

  def test_background_job_processing_performance
    puts "  Testing background job processing performance..."
    
    # Test 1: Job Processing Rate
    processing_rate = test_job_processing_rate
    
    # Test 2: Queue Management Performance
    queue_performance = test_job_queue_performance
    
    # Test 3: Job Retry and Error Handling
    retry_performance = test_job_retry_performance
    
    @performance_results[:background_jobs] = {
      processing_rate: processing_rate,
      queue_performance: queue_performance,
      retry_performance: retry_performance
    }
  end

  def test_job_processing_rate
    puts "    Testing job processing rate..."
    
    job_count = 100
    
    # Enqueue test jobs
    job_count.times do |i|
      SocialMediaSyncJob.perform_later(@test_brand.id)
      Reports::ReportGenerationJob.perform_later(@custom_report.id) if @custom_report
    end
    
    processing_time = Benchmark.measure do
      perform_enqueued_jobs
    end
    
    jobs_per_second = (job_count * 2) / processing_time.real # *2 because we enqueue 2 types
    puts "    Job processing rate: #{jobs_per_second.round(2)} jobs/second"
    
    assert jobs_per_second >= PERFORMANCE_TARGETS[:job_processing_rate_per_second],
           "Job processing too slow: #{jobs_per_second.round(2)} jobs/second"
    
    jobs_per_second
  end

  def test_job_queue_performance
    puts "    Testing job queue management performance..."
    
    large_queue_size = 500
    
    # Fill queue with jobs
    queue_time = Benchmark.measure do
      large_queue_size.times do |i|
        SocialMediaSyncJob.perform_later(@test_brand.id)
      end
    end
    
    # Test queue processing
    processing_time = Benchmark.measure do
      perform_enqueued_jobs
    end
    
    total_queue_time = queue_time.real + processing_time.real
    puts "    Queue management time: #{total_queue_time.round(3)}s for #{large_queue_size} jobs"
    
    {
      queue_size: large_queue_size,
      queue_time: queue_time.real,
      processing_time: processing_time.real,
      total_time: total_queue_time
    }
  end

  def test_job_retry_performance
    puts "    Testing job retry performance..."
    
    retry_count = 10
    
    retry_time = Benchmark.measure do
      retry_count.times do |i|
        begin
          # Simulate failing job
          job = SocialMediaSyncJob.new
          job.perform(@test_brand.id)
        rescue => e
          # Job will be retried automatically
        end
      end
    end
    
    puts "    Job retry handling time: #{retry_time.real.round(3)}s"
    
    retry_time.real
  end

  # =============================================================================
  # DATABASE QUERY OPTIMIZATION PERFORMANCE TESTS
  # =============================================================================

  def test_database_query_optimization_performance
    puts "  Testing database query optimization performance..."
    
    # Create substantial test data
    create_large_analytics_dataset
    
    # Test 1: Basic Query Performance
    basic_queries = test_basic_query_performance
    
    # Test 2: Complex Aggregation Performance
    aggregation_queries = test_complex_aggregation_performance
    
    # Test 3: Join Query Performance
    join_queries = test_join_query_performance
    
    # Test 4: Bulk Insert Performance
    bulk_insert = test_bulk_insert_performance
    
    @performance_results[:database_performance] = {
      basic_queries: basic_queries,
      aggregation_queries: aggregation_queries,
      join_queries: join_queries,
      bulk_insert: bulk_insert
    }
  end

  def test_basic_query_performance
    puts "    Testing basic query performance..."
    
    queries = {
      social_media_count: -> { SocialMediaMetric.count },
      recent_metrics: -> { SocialMediaMetric.where("created_at > ?", 7.days.ago).count },
      platform_filter: -> { SocialMediaMetric.where(platform: "facebook").count },
      date_range: -> { GoogleAnalyticsMetric.where(date: 30.days.ago..Time.current).count }
    }
    
    query_times = {}
    
    queries.each do |query_name, query_proc|
      query_time = Benchmark.measure do
        5.times { query_proc.call }
      end
      
      avg_time = (query_time.real / 5) * 1000 # Convert to milliseconds
      query_times[query_name] = avg_time
      
      puts "    #{query_name}: #{avg_time.round(2)}ms average"
      
      assert avg_time <= PERFORMANCE_TARGETS[:query_execution_max_ms],
             "Query #{query_name} too slow: #{avg_time.round(2)}ms"
    end
    
    query_times
  end

  def test_complex_aggregation_performance
    puts "    Testing complex aggregation performance..."
    
    aggregation_time = Benchmark.measure do
      # Complex multi-platform aggregation query
      SocialMediaMetric.joins(:social_media_integration)
                      .where(social_media_integrations: { brand_id: @test_brand.id })
                      .group(:platform, :metric_type)
                      .group("DATE_TRUNC('day', social_media_metrics.date)")
                      .sum(:value)
    end
    
    puts "    Complex aggregation time: #{aggregation_time.real.round(3)}s"
    
    assert aggregation_time.real <= PERFORMANCE_TARGETS[:complex_aggregation_max_seconds],
           "Complex aggregation too slow: #{aggregation_time.real}s"
    
    aggregation_time.real
  end

  def test_join_query_performance
    puts "    Testing join query performance..."
    
    join_time = Benchmark.measure do
      # Multi-table join query
      SocialMediaMetric.joins(social_media_integration: :brand)
                      .joins("LEFT JOIN email_metrics ON email_metrics.date = social_media_metrics.date")
                      .where(brands: { id: @test_brand.id })
                      .select("social_media_metrics.*, email_metrics.value as email_value")
                      .limit(1000)
                      .to_a
    end
    
    puts "    Join query time: #{join_time.real.round(3)}s"
    
    join_time.real
  end

  def test_bulk_insert_performance
    puts "    Testing bulk insert performance..."
    
    record_count = 5000
    records = []
    
    record_count.times do |i|
      records << {
        social_media_integration_id: @facebook_integration.id,
        platform: "facebook",
        metric_type: "test_bulk",
        value: rand(1000),
        date: Date.current,
        created_at: Time.current,
        updated_at: Time.current
      }
    end
    
    insert_time = Benchmark.measure do
      SocialMediaMetric.insert_all(records)
    end
    
    inserts_per_second = record_count / insert_time.real
    puts "    Bulk insert rate: #{inserts_per_second.round(2)} records/second"
    
    assert inserts_per_second >= PERFORMANCE_TARGETS[:bulk_insert_per_second],
           "Bulk insert too slow: #{inserts_per_second.round(2)} records/second"
    
    inserts_per_second
  end

  # =============================================================================
  # ALERT SYSTEM SCALABILITY PERFORMANCE TESTS
  # =============================================================================

  def test_alert_system_scalability_performance
    puts "  Testing alert system scalability performance..."
    
    # Test 1: Alert Processing Performance
    alert_processing = test_alert_processing_performance
    
    # Test 2: Alert Delivery Performance
    alert_delivery = test_alert_delivery_performance
    
    # Test 3: Alert System Under Load
    alert_load = test_alert_system_under_load
    
    @performance_results[:alert_system] = {
      processing: alert_processing,
      delivery: alert_delivery,
      under_load: alert_load
    }
  end

  def test_alert_processing_performance
    puts "    Testing alert processing performance..."
    
    # Create performance thresholds
    threshold = PerformanceThreshold.create!(
      brand: @test_brand,
      metric_name: "engagement_rate",
      threshold_type: "min",
      threshold_value: 5.0,
      alert_enabled: true
    )
    
    # Create metrics that trigger alerts
    alert_count = 50
    
    processing_time = Benchmark.measure do
      alert_count.times do |i|
        SocialMediaMetric.create!(
          social_media_integration: @facebook_integration,
          metric_type: "engagement_rate",
          value: 2.0, # Below threshold
          date: Date.current
        )
      end
      
      # Process alerts
      perform_enqueued_jobs do
        Analytics::Alerts::MonitoringService.new.check_thresholds
      end
    end
    
    alerts_per_second = alert_count / processing_time.real
    puts "    Alert processing rate: #{alerts_per_second.round(2)} alerts/second"
    
    # Verify alerts were created
    assert AlertInstance.where(performance_threshold: threshold).exists?
    
    alerts_per_second
  end

  def test_alert_delivery_performance
    puts "    Testing alert delivery performance..."
    
    alert = PerformanceAlert.create!(
      brand: @test_brand,
      title: "Performance Test Alert",
      message: "Testing alert delivery performance",
      severity: "high",
      alert_type: "threshold"
    )
    
    delivery_time = Benchmark.measure do
      perform_enqueued_jobs do
        Analytics::Notifications::DeliveryService.new.deliver_alert(alert, channels: ["email"])
      end
    end
    
    puts "    Alert delivery time: #{delivery_time.real.round(3)}s"
    
    assert delivery_time.real <= PERFORMANCE_TARGETS[:alert_delivery_time_seconds],
           "Alert delivery too slow: #{delivery_time.real}s"
    
    delivery_time.real
  end

  def test_alert_system_under_load
    puts "    Testing alert system under load..."
    
    # Create multiple alerts simultaneously
    alert_count = 20
    alerts = []
    
    alert_count.times do |i|
      alerts << PerformanceAlert.new(
        brand: @test_brand,
        title: "Load Test Alert #{i}",
        message: "Testing system under load",
        severity: "medium",
        alert_type: "threshold"
      )
    end
    
    load_time = Benchmark.measure do
      # Bulk create alerts
      PerformanceAlert.insert_all(
        alerts.map { |alert| alert.attributes.except("id") }
      )
      
      # Process all alerts
      perform_enqueued_jobs do
        PerformanceAlert.where(title: "Load Test Alert%").find_each do |alert|
          Analytics::Notifications::DeliveryService.new.deliver_alert(alert, channels: ["email"])
        end
      end
    end
    
    puts "    Alert system load time: #{load_time.real.round(3)}s for #{alert_count} alerts"
    
    load_time.real
  end

  # =============================================================================
  # API RESPONSE TIME PERFORMANCE TESTS
  # =============================================================================

  def test_api_response_time_performance
    puts "  Testing API response time performance..."
    
    # Test 1: Analytics API Endpoints
    analytics_api = test_analytics_api_performance
    
    # Test 2: Metrics API Endpoints
    metrics_api = test_metrics_api_performance
    
    # Test 3: Aggregation API Endpoints
    aggregation_api = test_aggregation_api_performance
    
    @performance_results[:api_performance] = {
      analytics: analytics_api,
      metrics: metrics_api,
      aggregation: aggregation_api
    }
  end

  def test_analytics_api_performance
    puts "    Testing analytics API performance..."
    
    api_endpoints = [
      -> { get analytics_dashboard_path(@test_brand), xhr: true },
      -> { get analytics_dashboard_path(@test_brand), params: { time_range: "30d" }, xhr: true },
      -> { get analytics_dashboard_path(@test_brand), params: { platform: "facebook" }, xhr: true }
    ]
    
    response_times = []
    
    api_endpoints.each_with_index do |endpoint, index|
      response_time = Benchmark.measure do
        endpoint.call
        assert_response :success
      end
      
      response_times << response_time.real
      puts "    Analytics API endpoint #{index + 1}: #{response_time.real.round(3)}s"
      
      assert response_time.real <= PERFORMANCE_TARGETS[:analytics_api_response_seconds],
             "Analytics API too slow: #{response_time.real}s"
    end
    
    response_times
  end

  def test_metrics_api_performance
    puts "    Testing metrics API performance..."
    
    response_time = Benchmark.measure do
      # Simulate metrics API call
      get analytics_dashboard_path(@test_brand), xhr: true, params: {
        action: "metrics",
        platforms: ["facebook", "instagram"],
        metrics: ["reach", "engagement"],
        time_range: "7d"
      }
      assert_response :success
    end
    
    puts "    Metrics API response time: #{response_time.real.round(3)}s"
    
    assert response_time.real <= PERFORMANCE_TARGETS[:metrics_api_response_seconds],
           "Metrics API too slow: #{response_time.real}s"
    
    response_time.real
  end

  def test_aggregation_api_performance
    puts "    Testing aggregation API performance..."
    
    response_time = Benchmark.measure do
      # Simulate complex aggregation API call
      get analytics_dashboard_path(@test_brand), xhr: true, params: {
        action: "aggregate",
        group_by: "platform",
        time_period: "daily",
        time_range: "30d"
      }
      assert_response :success
    end
    
    puts "    Aggregation API response time: #{response_time.real.round(3)}s"
    
    assert response_time.real <= PERFORMANCE_TARGETS[:aggregation_api_response_seconds],
           "Aggregation API too slow: #{response_time.real}s"
    
    response_time.real
  end

  # =============================================================================
  # MEMORY AND RESOURCE USAGE TESTS
  # =============================================================================

  def test_memory_and_resource_usage
    puts "  Testing memory and resource usage..."
    
    initial_memory = get_memory_usage
    
    # Perform memory-intensive operations
    create_large_analytics_dataset
    test_concurrent_dashboard_users
    
    final_memory = get_memory_usage
    memory_increase = final_memory - initial_memory
    
    puts "    Memory increase: #{memory_increase.round(2)}MB"
    
    assert memory_increase <= PERFORMANCE_TARGETS[:max_memory_increase_mb],
           "Memory usage too high: #{memory_increase.round(2)}MB"
    
    @performance_results[:memory_usage] = {
      initial: initial_memory,
      final: final_memory,
      increase: memory_increase
    }
  end

  # =============================================================================
  # STRESS TESTING AND SYSTEM RESILIENCE
  # =============================================================================

  def test_stress_and_resilience
    puts "  Testing system stress and resilience..."
    
    # Test 1: System Under Maximum Load
    max_load = test_system_under_maximum_load
    
    # Test 2: Error Recovery Under Stress
    error_recovery = test_error_recovery_under_stress
    
    # Test 3: Performance Degradation Analysis
    degradation = test_performance_degradation
    
    @performance_results[:stress_testing] = {
      max_load: max_load,
      error_recovery: error_recovery,
      degradation: degradation
    }
  end

  def test_system_under_maximum_load
    puts "    Testing system under maximum load..."
    
    stress_time = Benchmark.measure do
      # Simulate maximum concurrent operations
      threads = []
      
      # High-volume data processing
      threads << Thread.new { process_social_media_metrics_batch(1000) }
      threads << Thread.new { process_email_metrics_batch(1000) }
      
      # Multiple dashboard users
      5.times do |i|
        threads << Thread.new do
          get analytics_dashboard_path(@test_brand), headers: { "User-Id" => "stress_user_#{i}" }
        end
      end
      
      # Background job processing
      threads << Thread.new { perform_enqueued_jobs }
      
      # WebSocket connections
      threads << Thread.new do
        connect "/cable"
        subscribe_to_channel("AnalyticsDashboardChannel", brand_id: @test_brand.id)
      end
      
      threads.each(&:join)
    end
    
    puts "    Maximum load test time: #{stress_time.real.round(3)}s"
    
    stress_time.real
  end

  def test_error_recovery_under_stress
    puts "    Testing error recovery under stress..."
    
    # Simulate various error conditions under load
    recovery_time = Benchmark.measure do
      begin
        # Simulate database connection issues
        # (In real test, this would use actual database failure simulation)
        
        # Test ETL pipeline recovery
        perform_enqueued_jobs do
          Etl::PipelineHealthMonitorJob.perform_now
        end
        
        # Test alert system recovery
        Analytics::Alerts::MonitoringService.new.check_thresholds
        
      rescue => e
        puts "      Error during stress test: #{e.message}"
      end
    end
    
    puts "    Error recovery time: #{recovery_time.real.round(3)}s"
    
    recovery_time.real
  end

  def test_performance_degradation
    puts "    Testing performance degradation under increasing load..."
    
    load_levels = [10, 25, 50, 75, 100]
    degradation_data = {}
    
    load_levels.each do |load_percent|
      operations = (load_percent / 10.0).round
      
      response_time = Benchmark.measure do
        operations.times do
          get analytics_dashboard_path(@test_brand), xhr: true
        end
      end
      
      avg_response_time = response_time.real / operations
      degradation_data[load_percent] = avg_response_time
      
      puts "    Load #{load_percent}%: #{avg_response_time.round(3)}s average response"
    end
    
    degradation_data
  end

  # =============================================================================
  # PERFORMANCE ANALYSIS AND REPORTING
  # =============================================================================

  def analyze_comprehensive_performance_results
    puts "\n" + "="*100
    puts "COMPREHENSIVE PERFORMANCE ANALYSIS"
    puts "="*100
    
    total_duration = @end_time - @start_time
    puts "Total performance test duration: #{total_duration.round(2)} seconds"
    
    # Analyze each performance area
    analyze_performance_area("High-Volume Data Processing", :high_volume_processing)
    analyze_performance_area("Dashboard Performance", :dashboard_performance)
    analyze_performance_area("WebSocket Performance", :websocket_performance)
    analyze_performance_area("ETL Pipeline", :etl_pipeline)
    analyze_performance_area("Background Jobs", :background_jobs)
    analyze_performance_area("Database Performance", :database_performance)
    analyze_performance_area("Alert System", :alert_system)
    analyze_performance_area("API Performance", :api_performance)
    
    # Overall performance assessment
    overall_assessment = assess_overall_performance
    
    # Generate recommendations
    generate_optimization_recommendations
    
    # Validate against performance targets
    validate_performance_targets
    
    puts "\nüéØ PERFORMANCE TEST SUMMARY"
    puts "="*50
    puts overall_assessment
  end

  def analyze_performance_area(area_name, results_key)
    puts "\nüìä #{area_name}:"
    results = @performance_results[results_key]
    
    if results
      case results_key
      when :high_volume_processing
        puts "  ‚úì Processing Rate: #{results[:processing_rate].round(2)} records/second"
        puts "  ‚úì Daily Capacity: #{results[:daily_capacity].round(0)} records"
        status = results[:processing_rate] >= PERFORMANCE_TARGETS[:high_volume_processing_per_second] ? "PASS" : "FAIL"
        puts "  Status: #{status}"
        
      when :dashboard_performance
        puts "  ‚úì Load Time: #{results[:load_time].round(3)}s"
        puts "  ‚úì Refresh Time: #{results[:refresh_time].round(3)}s"
        puts "  ‚úì Concurrent Users Success Rate: #{results[:concurrent_users][:success_rate]}%"
        load_status = results[:load_time] <= PERFORMANCE_TARGETS[:dashboard_load_time_seconds] ? "PASS" : "FAIL"
        puts "  Status: #{load_status}"
        
      when :websocket_performance
        puts "  ‚úì Connection Time: #{results[:connection_time].round(2)}ms"
        puts "  ‚úì Message Latency: #{results[:message_latency].round(2)}ms"
        puts "  ‚úì Concurrent Connections: #{results[:concurrent_connections][:success_rate]}%"
        ws_status = results[:connection_time] <= PERFORMANCE_TARGETS[:websocket_connection_establishment_ms] ? "PASS" : "FAIL"
        puts "  Status: #{ws_status}"
        
      when :etl_pipeline
        puts "  ‚úì Full Pipeline Time: #{results[:full_pipeline_time].round(2)}s"
        puts "  ‚úì Real-time Processing: #{results[:realtime_performance][:avg_processing_time].round(2)}ms"
        etl_status = results[:full_pipeline_time] <= PERFORMANCE_TARGETS[:etl_pipeline_max_seconds] ? "PASS" : "FAIL"
        puts "  Status: #{etl_status}"
        
      when :api_performance
        puts "  ‚úì Analytics API: #{results[:analytics].max.round(3)}s max"
        puts "  ‚úì Metrics API: #{results[:metrics].round(3)}s"
        puts "  ‚úì Aggregation API: #{results[:aggregation].round(3)}s"
        api_status = results[:analytics].max <= PERFORMANCE_TARGETS[:analytics_api_response_seconds] ? "PASS" : "FAIL"
        puts "  Status: #{api_status}"
      end
    else
      puts "  ‚ö†Ô∏è  No results available"
    end
  end

  def assess_overall_performance
    passing_areas = 0
    total_areas = 0
    
    critical_tests = [
      :high_volume_processing,
      :dashboard_performance,
      :websocket_performance,
      :etl_pipeline,
      :api_performance
    ]
    
    critical_tests.each do |test|
      total_areas += 1
      results = @performance_results[test]
      
      case test
      when :high_volume_processing
        passing_areas += 1 if results && results[:processing_rate] >= PERFORMANCE_TARGETS[:high_volume_processing_per_second]
      when :dashboard_performance
        passing_areas += 1 if results && results[:load_time] <= PERFORMANCE_TARGETS[:dashboard_load_time_seconds]
      when :etl_pipeline
        passing_areas += 1 if results && results[:full_pipeline_time] <= PERFORMANCE_TARGETS[:etl_pipeline_max_seconds]
      when :api_performance
        passing_areas += 1 if results && results[:analytics].max <= PERFORMANCE_TARGETS[:analytics_api_response_seconds]
      else
        passing_areas += 1 # Default to passing for areas without specific validation
      end
    end
    
    success_rate = (passing_areas.to_f / total_areas * 100).round(2)
    
    if success_rate >= 90
      "üéâ EXCELLENT PERFORMANCE - All systems meeting enterprise requirements (#{success_rate}%)"
    elsif success_rate >= 75
      "‚úÖ GOOD PERFORMANCE - Most systems meeting requirements with minor optimizations needed (#{success_rate}%)"
    elsif success_rate >= 50
      "‚ö†Ô∏è  ACCEPTABLE PERFORMANCE - Significant optimizations needed (#{success_rate}%)"
    else
      "‚ùå POOR PERFORMANCE - Major performance issues require immediate attention (#{success_rate}%)"
    end
  end

  def generate_optimization_recommendations
    puts "\nüîß OPTIMIZATION RECOMMENDATIONS"
    puts "="*50
    
    recommendations = []
    
    # Analyze each performance area for optimization opportunities
    if @performance_results[:high_volume_processing]
      rate = @performance_results[:high_volume_processing][:processing_rate]
      if rate < PERFORMANCE_TARGETS[:high_volume_processing_per_second]
        recommendations << "Implement batch processing optimization for high-volume data ingestion"
        recommendations << "Consider implementing data partitioning and parallel processing"
      end
    end
    
    if @performance_results[:dashboard_performance]
      load_time = @performance_results[:dashboard_performance][:load_time]
      if load_time > 2.0
        recommendations << "Implement dashboard data caching and lazy loading"
        recommendations << "Optimize database queries with proper indexing"
      end
    end
    
    if @performance_results[:websocket_performance]
      connection_time = @performance_results[:websocket_performance][:connection_time]
      if connection_time > 300
        recommendations << "Optimize WebSocket connection pooling and management"
        recommendations << "Implement connection load balancing"
      end
    end
    
    if @performance_results[:etl_pipeline]
      pipeline_time = @performance_results[:etl_pipeline][:full_pipeline_time]
      if pipeline_time > 180
        recommendations << "Implement ETL pipeline parallelization"
        recommendations << "Optimize data transformation algorithms"
      end
    end
    
    if @performance_results[:database_performance]
      recommendations << "Review and optimize database indexes based on query patterns"
      recommendations << "Consider implementing read replicas for analytics queries"
    end
    
    # General recommendations
    recommendations << "Implement comprehensive monitoring and alerting for performance metrics"
    recommendations << "Set up automated performance regression testing"
    recommendations << "Consider implementing CDN for static assets"
    recommendations << "Implement application-level caching (Redis/Memcached)"
    
    if recommendations.empty?
      puts "‚úì No specific optimizations needed - performance is within target ranges"
    else
      recommendations.each_with_index do |rec, index|
        puts "#{index + 1}. #{rec}"
      end
    end
  end

  def validate_performance_targets
    puts "\nüéØ PERFORMANCE TARGET VALIDATION"
    puts "="*50
    
    validations = [
      {
        name: "Dashboard Load Time",
        target: "< #{PERFORMANCE_TARGETS[:dashboard_load_time_seconds]}s",
        actual: @performance_results.dig(:dashboard_performance, :load_time),
        unit: "s"
      },
      {
        name: "API Response Time",
        target: "< #{PERFORMANCE_TARGETS[:analytics_api_response_seconds]}s",
        actual: @performance_results.dig(:api_performance, :analytics)&.max,
        unit: "s"
      },
      {
        name: "Alert Delivery Time",
        target: "< #{PERFORMANCE_TARGETS[:alert_delivery_time_seconds]}s",
        actual: @performance_results.dig(:alert_system, :delivery),
        unit: "s"
      },
      {
        name: "High-Volume Processing",
        target: "> #{PERFORMANCE_TARGETS[:high_volume_processing_per_second]} records/s",
        actual: @performance_results.dig(:high_volume_processing, :processing_rate),
        unit: " records/s"
      },
      {
        name: "Concurrent Users",
        target: "100+ users supported",
        actual: @performance_results.dig(:dashboard_performance, :concurrent_users, :success_rate),
        unit: "% success rate"
      }
    ]
    
    all_targets_met = true
    
    validations.each do |validation|
      if validation[:actual]
        status = case validation[:name]
                when "Dashboard Load Time"
                  validation[:actual] <= PERFORMANCE_TARGETS[:dashboard_load_time_seconds] ? "‚úÖ PASS" : "‚ùå FAIL"
                when "API Response Time"
                  validation[:actual] <= PERFORMANCE_TARGETS[:analytics_api_response_seconds] ? "‚úÖ PASS" : "‚ùå FAIL"
                when "Alert Delivery Time"
                  validation[:actual] <= PERFORMANCE_TARGETS[:alert_delivery_time_seconds] ? "‚úÖ PASS" : "‚ùå FAIL"
                when "High-Volume Processing"
                  validation[:actual] >= PERFORMANCE_TARGETS[:high_volume_processing_per_second] ? "‚úÖ PASS" : "‚ùå FAIL"
                when "Concurrent Users"
                  validation[:actual] >= 95 ? "‚úÖ PASS" : "‚ùå FAIL"
                else
                  "‚ö†Ô∏è  UNKNOWN"
                end
        
        puts "#{validation[:name]}: #{validation[:actual].round(3)}#{validation[:unit]} (Target: #{validation[:target]}) #{status}"
        
        if status.include?("FAIL")
          all_targets_met = false
        end
      else
        puts "#{validation[:name]}: No data available ‚ö†Ô∏è"
        all_targets_met = false
      end
    end
    
    puts "\n" + "="*50
    if all_targets_met
      puts "üéâ ALL PERFORMANCE TARGETS MET - System ready for production with 100+ concurrent users"
    else
      puts "‚ö†Ô∏è  PERFORMANCE TARGETS NOT MET - Review failed areas before production deployment"
    end
  end

  def generate_comprehensive_performance_report
    report_data = {
      test_suite: "Analytics Monitoring Performance Test Suite",
      execution_time: @start_time.iso8601,
      total_duration: @end_time - @start_time,
      performance_targets: PERFORMANCE_TARGETS,
      results: @performance_results,
      summary: {
        tests_executed: @performance_results.keys.size,
        critical_systems_tested: [
          "High-Volume Data Processing (1M+ records/day)",
          "Dashboard Performance (<3s load time)",
          "API Response Times (<2s)",
          "Alert Delivery (<1min)",
          "WebSocket Real-time Performance",
          "ETL Pipeline Processing",
          "Database Query Optimization",
          "Concurrent User Support (100+ users)"
        ]
      },
      environment: {
        rails_version: Rails.version,
        ruby_version: RUBY_VERSION,
        database: ActiveRecord::Base.connection.adapter_name,
        test_environment: Rails.env,
        timestamp: Time.current.iso8601
      },
      recommendations: generate_recommendations_data
    }
    
    # Save comprehensive report
    report_filename = "analytics_performance_test_#{@start_time.strftime('%Y%m%d_%H%M%S')}.json"
    report_path = Rails.root.join("tmp", report_filename)
    File.write(report_path, JSON.pretty_generate(report_data))
    
    # Generate HTML report
    html_report_path = generate_html_performance_report(report_data)
    
    # Generate CSV summary
    csv_report_path = generate_csv_performance_summary(report_data)
    
    puts "\nüìä COMPREHENSIVE PERFORMANCE REPORTS GENERATED:"
    puts "JSON Report: #{report_path}"
    puts "HTML Report: #{html_report_path}"
    puts "CSV Summary: #{csv_report_path}"
    puts "\nUse these reports for performance analysis, optimization planning, and stakeholder communication."
  end

  def generate_recommendations_data
    # This would contain structured recommendation data
    # Implementation details would depend on specific requirements
    {
      high_priority: [],
      medium_priority: [],
      low_priority: [],
      monitoring: []
    }
  end

  def generate_html_performance_report(report_data)
    html_content = <<~HTML
      <!DOCTYPE html>
      <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Analytics Monitoring Performance Test Report</title>
        <style>
          body { font-family: Arial, sans-serif; margin: 20px; }
          .header { background: #f8f9fa; padding: 20px; border-radius: 8px; }
          .section { margin: 20px 0; padding: 15px; border: 1px solid #dee2e6; border-radius: 5px; }
          .pass { color: #28a745; }
          .fail { color: #dc3545; }
          .warning { color: #ffc107; }
          table { width: 100%; border-collapse: collapse; margin: 10px 0; }
          th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }
          th { background-color: #f2f2f2; }
        </style>
      </head>
      <body>
        <div class="header">
          <h1>Analytics Monitoring Performance Test Report</h1>
          <p><strong>Execution Time:</strong> #{report_data[:execution_time]}</p>
          <p><strong>Total Duration:</strong> #{report_data[:total_duration].round(2)} seconds</p>
        </div>
        
        <div class="section">
          <h2>Performance Summary</h2>
          <p>This report validates the analytics monitoring system's ability to handle enterprise-scale loads including:</p>
          <ul>
            #{report_data[:summary][:critical_systems_tested].map { |system| "<li>#{system}</li>" }.join}
          </ul>
        </div>
        
        <div class="section">
          <h2>Test Results</h2>
          <!-- Test results would be formatted here -->
          <p>Detailed results are available in the JSON report for programmatic analysis.</p>
        </div>
        
        <div class="section">
          <h2>Environment Information</h2>
          <table>
            <tr><th>Component</th><th>Version</th></tr>
            <tr><td>Rails</td><td>#{report_data[:environment][:rails_version]}</td></tr>
            <tr><td>Ruby</td><td>#{report_data[:environment][:ruby_version]}</td></tr>
            <tr><td>Database</td><td>#{report_data[:environment][:database]}</td></tr>
          </table>
        </div>
      </body>
      </html>
    HTML
    
    html_path = Rails.root.join("tmp", "analytics_performance_report_#{@start_time.strftime('%Y%m%d_%H%M%S')}.html")
    File.write(html_path, html_content)
    html_path
  end

  def generate_csv_performance_summary(report_data)
    csv_content = "Test Area,Metric,Target,Actual,Status\n"
    
    # Add performance data to CSV
    # This would be populated with actual performance metrics
    
    csv_path = Rails.root.join("tmp", "analytics_performance_summary_#{@start_time.strftime('%Y%m%d_%H%M%S')}.csv")
    File.write(csv_path, csv_content)
    csv_path
  end

  # =============================================================================
  # HELPER METHODS AND SETUP
  # =============================================================================

  def run_performance_test(test_name, critical: false)
    puts "\n#{'-'*80}"
    puts "Testing: #{test_name} #{'(CRITICAL)' if critical}"
    puts "#{'-'*80}"
    
    initial_memory = get_memory_usage
    start_time = Time.current
    
    begin
      yield
    rescue => e
      puts "‚ùå Test failed: #{e.message}"
      if critical
        raise e
      end
    end
    
    end_time = Time.current
    final_memory = get_memory_usage
    
    @performance_results[test_name.downcase.gsub(/\s+/, '_').to_sym] = {
      duration: end_time - start_time,
      memory_increase: final_memory - initial_memory,
      timestamp: start_time,
      critical: critical
    }
    
    puts "‚úì #{test_name} completed in #{(end_time - start_time).round(2)}s"
    puts "  Memory increase: #{(final_memory - initial_memory).round(2)}MB"
  end

  def setup_test_integrations
    # Create test integrations for performance testing
    @facebook_integration = SocialMediaIntegration.create!(
      brand: @test_brand,
      platform: "facebook",
      access_token: "test_token",
      status: "active",
      platform_account_id: "test_account_123"
    )
    
    @email_integration = EmailIntegration.create!(
      brand: @test_brand,
      platform: "mailchimp",
      api_key: "test_api_key",
      status: "active"
    )
    
    @email_campaign = EmailCampaign.create!(
      brand: @test_brand,
      name: "Performance Test Campaign",
      platform: "mailchimp",
      campaign_id: "perf_campaign_123"
    )
    
    @crm_integration = CrmIntegration.create!(
      brand: @test_brand,
      platform: "salesforce",
      access_token: "test_crm_token",
      status: "active"
    )
    
    @custom_report = CustomReport.create!(
      name: "Performance Test Report",
      brand: @test_brand,
      user: @test_user,
      report_type: "dashboard",
      configuration: { metrics: ["reach", "engagement"] }.to_json
    )
  end

  def stub_high_volume_platform_responses
    # Mock high-volume responses for performance testing
    large_data_response = {
      data: Array.new(1000) { |i|
        {
          name: "reach",
          values: [{ value: rand(1000..10000), end_time: (Time.current - i.hours).iso8601 }]
        }
      }
    }
    
    stub_request(:get, /graph\.facebook\.com|api\.instagram\.com|api\.twitter\.com/)
      .to_return(status: 200, body: large_data_response.to_json)
    
    stub_request(:get, /api\.mailchimp\.com|api\.sendgrid\.com/)
      .to_return(status: 200, body: {
        campaigns: Array.new(500) { |i|
          {
            id: "campaign_#{i}",
            report_summary: { opens: rand(100..1000), clicks: rand(10..100) }
          }
        }
      }.to_json)
  end

  def stub_comprehensive_etl_data_sources
    stub_request(:post, "https://analyticsreporting.googleapis.com/v4/reports:batchGet")
      .to_return(status: 200, body: {
        reports: [{
          data: {
            rows: Array.new(100) { |i|
              {
                dimensions: [(Date.current - i.days).strftime("%Y%m%d")],
                metrics: [{ values: [rand(1000..5000).to_s, rand(100..500).to_s] }]
              }
            }
          }
        }]
      }.to_json)
  end

  def generate_etl_test_data
    {
      social_media: {
        facebook: Array.new(100) { { reach: rand(1000..5000), engagement: rand(100..500) } },
        instagram: Array.new(100) { { reach: rand(500..2500), engagement: rand(50..250) } }
      },
      email_marketing: {
        sent: rand(10000..50000),
        opened: rand(1000..5000),
        clicked: rand(100..500)
      },
      google_analytics: {
        sessions: rand(5000..25000),
        pageviews: rand(15000..75000),
        users: rand(3000..15000)
      }
    }
  end

  def generate_transformed_etl_data
    {
      format: "standardized",
      unified_metrics: {
        reach: rand(5000..25000),
        engagement: rand(500..2500),
        conversions: rand(50..250)
      },
      platform_breakdown: {
        facebook: { reach: rand(2000..10000) },
        instagram: { reach: rand(1000..5000) },
        email: { sent: rand(5000..25000) }
      },
      timestamp: Time.current.iso8601
    }
  end

  def create_realistic_analytics_data
    # Create realistic data for dashboard testing
    30.times do |i|
      date = i.days.ago.to_date
      
      # Social media metrics
      SocialMediaMetric.create!(
        social_media_integration: @facebook_integration,
        platform: "facebook",
        metric_type: "reach",
        value: rand(1000..5000),
        date: date
      )
      
      # Google Analytics metrics
      GoogleAnalyticsMetric.create!(
        brand: @test_brand,
        metric_name: "sessions",
        metric_value: rand(500..2500),
        date: date
      )
      
      # Email metrics
      EmailMetric.create!(
        email_campaign: @email_campaign,
        metric_type: "opened",
        value: rand(100..500),
        date: date
      )
    end
  end

  def create_large_analytics_dataset
    # Create a large dataset for database performance testing
    platforms = ["facebook", "instagram", "twitter", "linkedin"]
    metric_types = ["reach", "engagement", "impressions", "clicks"]
    
    bulk_data = []
    
    1000.times do |i|
      platforms.each do |platform|
        metric_types.each do |metric_type|
          bulk_data << {
            social_media_integration_id: @facebook_integration.id,
            platform: platform,
            metric_type: metric_type,
            value: rand(100..10000),
            date: rand(90.days).seconds.ago.to_date,
            created_at: Time.current,
            updated_at: Time.current
          }
        end
      end
    end
    
    SocialMediaMetric.insert_all(bulk_data)
    
    # Create Google Analytics data
    ga_bulk_data = []
    500.times do |i|
      ["sessions", "pageviews", "users", "bounce_rate"].each do |metric|
        ga_bulk_data << {
          brand_id: @test_brand.id,
          metric_name: metric,
          metric_value: rand(100..10000).to_f,
          date: rand(90.days).seconds.ago.to_date,
          created_at: Time.current,
          updated_at: Time.current
        }
      end
    end
    
    GoogleAnalyticsMetric.insert_all(ga_bulk_data)
  end

  def get_memory_usage
    `ps -o rss= -p #{Process.pid}`.to_i / 1024.0
  rescue
    0
  end
end