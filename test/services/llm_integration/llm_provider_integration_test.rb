require 'test_helper'

class LlmProviderIntegrationTest < ActiveSupport::TestCase
  def setup
    @openai_service = LlmService.new(model: "gpt-4-turbo-preview")
    @claude_service = LlmService.new(model: "claude-3-opus-20240229")
    # @multi_provider_service = LlmIntegration::MultiProviderService.new
  end

  test "should support multiple LLM providers with unified interface" do
    # Test multi-provider abstraction layer
    providers = @multi_provider_service.available_providers
    assert_includes providers, :openai
    assert_includes providers, :anthropic
    assert_includes providers, :cohere
    assert_includes providers, :huggingface
    
    # Test provider selection based on model
    assert_equal :openai, @multi_provider_service.detect_provider("gpt-4-turbo")
    assert_equal :anthropic, @multi_provider_service.detect_provider("claude-3-opus")
    assert_equal :cohere, @multi_provider_service.detect_provider("command-r-plus")
  end

  test "should handle provider failover gracefully" do
    # Test primary provider failure and automatic failover
    @multi_provider_service.configure(
      primary: :openai,
      fallback: [:anthropic, :cohere],
      timeout: 30
    )
    
    # Simulate OpenAI failure
    @multi_provider_service.expects(:call_provider).with(:openai, anything)
                           .raises(LlmIntegration::ProviderError.new("API unavailable"))
    @multi_provider_service.expects(:call_provider).with(:anthropic, anything)
                           .returns("Content generated by Claude")
    
    result = @multi_provider_service.generate_content("Write a marketing email")
    assert_equal "Content generated by Claude", result[:content]
    assert_equal :anthropic, result[:provider_used]
    assert result[:failover_occurred]
  end

  test "should implement circuit breaker pattern for provider reliability" do
    circuit_breaker = LlmIntegration::CircuitBreaker.new(
      failure_threshold: 3,
      timeout_duration: 60,
      retry_timeout: 300
    )
    
    # Test circuit breaker states
    assert_equal :closed, circuit_breaker.state
    
    # Simulate failures to trigger circuit breaker
    3.times do
      circuit_breaker.record_failure
    end
    
    assert_equal :open, circuit_breaker.state
    assert_raises(LlmIntegration::CircuitBreakerOpenError) do
      circuit_breaker.call { "This should fail" }
    end
  end

  test "should handle rate limiting with exponential backoff" do
    rate_limiter = LlmIntegration::RateLimiter.new(
      requests_per_minute: 60,
      requests_per_hour: 3000,
      backoff_strategy: :exponential
    )
    
    # Test rate limit tracking
    assert rate_limiter.can_make_request?
    
    # Simulate rate limit hit
    rate_limiter.expects(:requests_in_last_minute).returns(60)
    refute rate_limiter.can_make_request?
    
    # Test backoff calculation
    backoff_time = rate_limiter.calculate_backoff(attempt: 3)
    assert_in_delta 8.0, backoff_time, 1.0  # 2^3 = 8 seconds
  end

  test "should support provider-specific authentication methods" do
    # Test OpenAI authentication
    openai_auth = LlmIntegration::Authentication::OpenAIAuth.new
    headers = openai_auth.build_headers
    assert_equal "Bearer #{ENV['OPENAI_API_KEY']}", headers['Authorization']
    
    # Test Anthropic authentication
    anthropic_auth = LlmIntegration::Authentication::AnthropicAuth.new
    headers = anthropic_auth.build_headers
    assert_equal ENV['ANTHROPIC_API_KEY'], headers['x-api-key']
    assert_equal '2023-06-01', headers['anthropic-version']
  end

  test "should manage API keys securely with rotation support" do
    key_manager = LlmIntegration::ApiKeyManager.new
    
    # Test key validation
    assert key_manager.valid_key?(:openai, ENV['OPENAI_API_KEY'])
    refute key_manager.valid_key?(:openai, "invalid_key")
    
    # Test key rotation capability
    new_key = "new_api_key_123"
    key_manager.rotate_key(:openai, new_key)
    assert_equal new_key, key_manager.current_key(:openai)
    
    # Test key expiration tracking
    key_manager.set_key_expiry(:openai, 30.days.from_now)
    assert key_manager.key_expires_soon?(:openai, within: 7.days)
  end

  test "should provide detailed error handling and recovery" do
    error_handler = LlmIntegration::ErrorHandler.new
    
    # Test different error types
    auth_error = LlmIntegration::AuthenticationError.new("Invalid API key")
    rate_limit_error = LlmIntegration::RateLimitError.new("Rate limit exceeded")
    server_error = LlmIntegration::ServerError.new("Internal server error")
    
    # Test error classification
    assert error_handler.retryable?(rate_limit_error)
    assert error_handler.retryable?(server_error)
    refute error_handler.retryable?(auth_error)
    
    # Test recovery strategies
    recovery = error_handler.suggest_recovery(rate_limit_error)
    assert_equal :wait_and_retry, recovery[:strategy]
    assert recovery[:wait_time] > 0
  end

  test "should track provider performance metrics" do
    metrics = LlmIntegration::ProviderMetrics.new
    
    # Record successful request
    metrics.record_request(:openai, duration: 2.5, tokens: 150, success: true)
    
    # Test metrics calculation
    stats = metrics.provider_stats(:openai)
    assert_equal 1, stats[:total_requests]
    assert_equal 100.0, stats[:success_rate]
    assert_equal 2.5, stats[:avg_response_time]
    assert_equal 150, stats[:total_tokens_used]
  end

  test "should support different response formats across providers" do
    # Test OpenAI response parsing
    openai_response = {
      "choices" => [
        {
          "message" => {
            "content" => "Generated content"
          }
        }
      ]
    }
    
    parser = LlmIntegration::ResponseParser.new(:openai)
    content = parser.extract_content(openai_response)
    assert_equal "Generated content", content
    
    # Test Anthropic response parsing
    claude_response = {
      "content" => [
        {
          "text" => "Claude generated content"
        }
      ]
    }
    
    parser = LlmIntegration::ResponseParser.new(:anthropic)
    content = parser.extract_content(claude_response)
    assert_equal "Claude generated content", content
  end

  test "should implement request queuing for high-volume scenarios" do
    queue = LlmIntegration::RequestQueue.new(
      max_concurrent: 10,
      queue_limit: 100,
      priority_levels: 3
    )
    
    # Test request queueing
    request = LlmIntegration::ContentRequest.new(
      prompt: "Generate content",
      priority: :high,
      callback: ->(result) { puts result }
    )
    
    assert queue.enqueue(request)
    assert_equal 1, queue.size
    assert_equal :high, queue.peek.priority
  end

  test "should validate provider configurations" do
    validator = LlmIntegration::ConfigValidator.new
    
    valid_config = {
      provider: :openai,
      model: "gpt-4-turbo",
      api_key: "sk-test123",
      timeout: 30,
      max_tokens: 2000
    }
    
    invalid_config = {
      provider: :invalid_provider,
      model: nil,
      timeout: -1
    }
    
    assert validator.valid?(valid_config)
    refute validator.valid?(invalid_config)
    
    errors = validator.validate(invalid_config)
    assert_includes errors, "Invalid provider: invalid_provider"
    assert_includes errors, "Model cannot be nil"
    assert_includes errors, "Timeout must be positive"
  end
end